import pandas as pd

# Load the CSV file
csv_path = "divorce.csv"
xlsx_path = "divorce.xlsx"

# Try loading both formats to determine which one works
try:
    df = pd.read_csv(csv_path)
except Exception as e_csv:
    try:
        df = pd.read_excel(xlsx_path)
    except Exception as e_xlsx:
        df = None

# Display basic information about the dataset
df.info(), df.head() if df is not None else "Failed to load dataset."



# Reload the dataset with proper delimiter handling
df = pd.read_csv(csv_path, delimiter=";")

# Display the first few rows to confirm correct parsing
df.head()




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load dataset (adjust file path if needed)
df = pd.read_csv("divorce.csv", delimiter=";")

# Split dataset into features and target variable
X = df.drop(columns=["Class"])
y = df["Class"]

# Split into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features for better model performance
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train a Random Forest Classifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

# Predict on test data
y_pred = model.predict(X_test_scaled)

# Compute accuracy
accuracy = accuracy_score(y_test, y_pred)

# Print structured classification report
report = classification_report(y_test, y_pred, output_dict=True)

print("\n### Model Performance ###")
print(f"Accuracy: {accuracy:.2%}\n")
print("### Classification Report ###")
print(f"{'Class':<10}{'Precision':<10}{'Recall':<10}{'F1-score':<10}{'Support'}")
# The for loop was indented too far, causing the IndentationError.
# It should be aligned with the other statements within the display_results function.
for label, metrics in report.items():
    if label in ['0', '1']:  # Only display relevant classes
        print(f"{label:<10}{metrics['precision']:<10.2f}{metrics['recall']:<10.2f}{metrics['f1-score']:<10.2f}{metrics['support']}")

# Compute and display confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("\n### Confusion Matrix ###")
print(pd.DataFrame(cm, index=["Actual Stay (0)", "Actual Split (1)"],
                   columns=["Predicted Stay (0)", "Predicted Split (1)"]))

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix as a heatmap
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Stay (0)", "Split (1)"], yticklabels=["Stay (0)", "Split (1)"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix Heatmap")
plt.show()

# Feature Importance Plot
feature_importances = model.feature_importances_
feature_names = X.columns

# Sort features by importance
sorted_idx = np.argsort(feature_importances)[::-1]
sorted_features = [feature_names[i] for i in sorted_idx]
sorted_importances = feature_importances[sorted_idx]

# Plot top 10 important features
plt.figure(figsize=(10, 6))
sns.barplot(x=sorted_importances[:10], y=sorted_features[:10], palette="viridis")
plt.xlabel("Feature Importance Score")
plt.ylabel("Features")
plt.title("Top 10 Important Features")
plt.show()




import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load dataset
csv_path = "divorce.csv"
xlsx_path = "divorce.xlsx"

try:
    df = pd.read_csv(csv_path, delimiter=";")
except Exception as e:
    try:
        df = pd.read_excel(xlsx_path)
    except Exception as e_xlsx:
        print("Failed to load dataset.")
        df = None

if df is not None:
    X = df.drop(columns=["Class"])
    y = df["Class"]

    # Test scenarios with different train-test splits
    test_sizes = [0.2, 0.3, 0.4]  # 80-20, 70-30, 60-40 splits

    for test_size in test_sizes:
        print(f"\n--- Training with {int((1 - test_size) * 100)}% training and {int(test_size * 100)}% testing data ---")

        # Split dataset
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

        # Standardize features
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # Train model
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train_scaled, y_train)

        # Predictions
        y_pred = model.predict(X_test_scaled)

        # Accuracy
        accuracy = accuracy_score(y_test, y_pred)
        print(f"Accuracy: {accuracy:.2%}")

        # Classification Report
        report = classification_report(y_test, y_pred)
        print("Classification Report:\n", report)

        # Confusion Matrix
        cm = confusion_matrix(y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                    xticklabels=["Stay (0)", "Split (1)"],
                    yticklabels=["Stay (0)", "Split (1)"])
        plt.title(f"Confusion Matrix ({int(test_size * 100)}% Test Data)")
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.show()

        # Feature Importance
        feature_importances = model.feature_importances_
        feature_names = X.columns

        sorted_idx = np.argsort(feature_importances)[::-1]
        sorted_features = [feature_names[i] for i in sorted_idx]
        sorted_importances = feature_importances[sorted_idx]

        plt.figure(figsize=(10, 6))
        sns.barplot(x=sorted_importances[:10], y=sorted_features[:10], palette="viridis")
        plt.xlabel("Importance")
        plt.ylabel("Feature")
        plt.title("Top 10 Important Features")
        plt.show()
else:
    print("Dataset is not loaded properly.")




import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load dataset
csv_path = "divorce.csv"
xlsx_path = "divorce.xlsx"

try:
    df = pd.read_csv(csv_path, delimiter=";")
except Exception as e:
    try:
        df = pd.read_excel(xlsx_path)
    except Exception as e_xlsx:
        print("Failed to load dataset.")
        df = None

if df is not None:
    # ------------------- Data Preprocessing -------------------

    # Check for missing values
    print("Missing values per column:\n", df.isnull().sum())

    # Remove duplicate rows if any
    df.drop_duplicates(inplace=True)

    # Convert any non-numeric data if present (for demonstration)
    for col in df.columns:
        if df[col].dtype == 'object':
            df[col] = pd.factorize(df[col])[0]

    # ------------------- Feature & Target Split -------------------

    X = df.drop(columns=["Class"])
    y = df["Class"]

    # ------------------- Testing with Different Scenarios -------------------
    test_sizes = [0.2, 0.3, 0.4]  # 80-20, 70-30, 60-40 splits

    for test_size in test_sizes:
        print(f"\n--- Training with {int((1 - test_size) * 100)}% training and {int(test_size * 100)}% testing data ---")

        # Split dataset
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

        # ------------------- Data Transformation -------------------
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # ------------------- Model Training -------------------
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train_scaled, y_train)

        # ------------------- Evaluation -------------------
        y_pred = model.predict(X_test_scaled)

        accuracy = accuracy_score(y_test, y_pred)
        print(f"Accuracy: {accuracy:.2%}")

        report = classification_report(y_test, y_pred)
        print("Classification Report:\n", report)

        # Confusion Matrix
        cm = confusion_matrix(y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                    xticklabels=["Stay (0)", "Split (1)"],
                    yticklabels=["Stay (0)", "Split (1)"])
        plt.title(f"Confusion Matrix ({int(test_size * 100)}% Test Data)")
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.show()

        # ------------------- Feature Importance -------------------
        feature_importances = model.feature_importances_
        feature_names = X.columns

        sorted_idx = np.argsort(feature_importances)[::-1]
        sorted_features = [feature_names[i] for i in sorted_idx]
        sorted_importances = feature_importances[sorted_idx]

        plt.figure(figsize=(10, 6))
        sns.barplot(x=sorted_importances[:10], y=sorted_features[:10], palette="viridis")
        plt.xlabel("Importance")
        plt.ylabel("Feature")
        plt.title("Top 10 Important Features")
        plt.show()
else:
    print("Dataset is not loaded properly.")





import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

# Load dataset
csv_path = "divorce.csv"
xlsx_path = "divorce.xlsx"

try:
    df = pd.read_csv(csv_path, delimiter=";")
except Exception as e:
    try:
        df = pd.read_excel(xlsx_path)
    except Exception as e_xlsx:
        print("Failed to load dataset.")
        df = None

if df is not None:
    # ------------------- Data Cleaning -------------------

    # Check for missing values
    print("Missing values per column:\n", df.isnull().sum())

    # Drop rows with missing values (if any)
    df.dropna(inplace=True)

    # Remove duplicate rows
    df.drop_duplicates(inplace=True)

    # Convert non-numeric columns to numeric (if any)
    for col in df.columns:
        if df[col].dtype == 'object':
            df[col] = pd.factorize(df[col])[0]

    # ------------------- Attribute Reduction -------------------
    # For demonstration, assume first 2 columns are IDs or unrelated meta-data
    if df.shape[1] > 2:
        df = df.iloc[:, 2:]

    # ------------------- Data Transformation -------------------

    X = df.drop(columns=["Class"])
    y = df["Class"]

    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)

    # Combine transformed features with target column
    df_transformed = pd.DataFrame(X_scaled, columns=X.columns)
    df_transformed["Class"] = y.reset_index(drop=True)

    # Save cleaned and transformed data to a new CSV file
    df_transformed.to_csv("cleaned_transformed_divorce.csv", index=False)

    print("Data cleaning, unnecessary attribute removal, and 0-1 transformation completed. File saved as 'cleaned_transformed_divorce.csv'.")
else:
    print("Dataset is not loaded properly.")




import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

# Load cleaned and transformed data
df = pd.read_csv("cleaned_transformed_divorce.csv")

# Features and target
X = df.drop(columns=["Class"])
y = df["Class"]

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Classifiers to evaluate
models = {
    "Logistic Regression": LogisticRegression(),
    "SVM": SVC(),
    "Decision Tree": DecisionTreeClassifier(),
    "KNN": KNeighborsClassifier(),
    "Random Forest": RandomForestClassifier()
}

# Evaluation loop
for name, model in models.items():
    print(f"\n========== {name} ==========")
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

    # Accuracy
    acc = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {acc:.2%}")

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=["Stay (0)", "Split (1)"],
                yticklabels=["Stay (0)", "Split (1)"])
    plt.title(f"{name} - Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.tight_layout()
    plt.show()





import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

# Load cleaned and transformed data
df = pd.read_csv("cleaned_transformed_divorce.csv")

# Split into features and target
X = df.drop(columns=["Class"])
y = df["Class"]

# Split into training and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Dictionary of models to test
models = {
    "Logistic Regression": LogisticRegression(),
    "Support Vector Machine": SVC(),
    "Decision Tree": DecisionTreeClassifier(),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Random Forest": RandomForestClassifier()
}

# Train and evaluate each model
print("Model Accuracies:\n")
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    acc = accuracy_score(y_test, y_pred)
    print(f"{name:25}: {acc:.2%}")

